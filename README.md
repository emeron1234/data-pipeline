# Data Pipeline - Databricks Asset Bundle (DAB) Project

The project demonstrates end-to-end expertise in ETL pipeline development, data quality validation and CI/CD automation which, deployed using **Databricks Asset Bundles (DAB)** for modern CI/CD practices.

### **Business Requirement** ###
Engineered a scalable data pipeline processing **Contact Information** and **Real Estate** datasets through a multi-layer architecture, ensuring data quality, compliance, and production readiness for enterprise analytics.

---

## ğŸš€ Quick Start

### **Project Highlights**
- Pipeline Architecture implementation: Bronze â†’ Silver â†’ Gold
- Automated CI/CD pipeline: using GithHub Action
- QA Framework: Smoke and Regression testing
- Modern Deployment: using Databricks Asset Bundles (DAB)
- Data Quality validation: performed at every stage of task run

### **CI/CD Deployment**
The GitHub Actions workflow automatically:
1. âœ… Installs correct Databricks CLI
2. âœ… Validates bundle configuration
3. âœ… Deploys to Databricks workspace
4. âœ… Runs validation jobs

---

## ğŸ“‹ Repository File Structure

```
data-pipeline/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ data_etl.yml            # CI/CD workflow (DAB-based)
â”œâ”€â”€ data_pipeline/              # Core application code
â”‚   â”œâ”€â”€ contact_info/
â”‚   â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ data_generation/
â”‚   â”œâ”€â”€ real_estate/
â”‚   â””â”€â”€ validation/
â”œâ”€â”€ databricks.yml              # DAB configuration (MAIN)
â””â”€â”€ setup.py                    # Python package setup
```

---

## ğŸ—ï¸ End-to-End Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA ENGINEERING PIPELINE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Data Sources                  ğŸ”„ Processing Layers              ğŸ“Š Analytics
     â”‚                                   â”‚                             â”‚
     â”œâ”€â–º Synthetic Data â”€â”€â”€â”€â”€â”€â”€â”€â–º ğŸŸ¦ Raw Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚   Generator                  â”‚ (Parquet Files)                  â”‚
     â”‚   (Faker)                    â”‚ - No transformation              â”‚
     â”‚                              â”‚ - Batch tracking                 â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â–¼                                  â”‚
     â”‚                         ğŸŸ§ Bronze Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                              â”‚ - Data cleansing                 â”‚
     â”‚                              â”‚ - Null filtering                 â”‚
     â”‚                              â”‚ - Special char removal           â”‚
     â”‚                              â”‚ - Phone standardization          â”‚
     â”‚                              â”‚ - Name normalization             â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â–¼                                  â”‚
     â”‚                         ğŸŸ¨ Silver Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                              â”‚ - Delta Lake tables              â”‚
     â”‚                              â”‚ - Schema evolution               â”‚
     â”‚                              â”‚ - Ready for analytics            â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â”‚                                  â”‚
     â”‚                              â–¼                                  â”‚
     â”‚                         ğŸŸ© Gold Zone (Future Planned) â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                                - Aggregations                   â”‚
     â”‚                                - Business metrics               â”‚
     â”‚                                - Feature engineering            â”‚
     â”‚                                                                 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    âœ… Validation Layer (Parallel)
                         â”‚
                         â”œâ”€â–º Smoke Tests (Fast)
                         â”‚   - Business rule validation (YAML-based queries)
                         â”‚   
                         â””â”€â–º Regression Tests (Comprehensive)
                             - Schema validation
                             - Row count checks
                             - Data comparison
```
                             
---

## ğŸ”§ Key End-to-End Pipeline

### ***Data Generation Module***
Built synthetic test data for development and testing:
```
# File: data_pipeline/data_generation/task/generate_data_task.py
def etl_process(**options):
    """Generate realistic synthetic data using Faker"""
    fake = Faker()
    
    # Intelligent batch ID management
    batch_id = batch_ids_processing(path)  # Auto-increments from last batch
    
    # Generate records with realistic patterns
    for i in range(num_rows):
        data.append({
            "profile_id": fake.uuid4(),
            "first_name": random_cases(fake.first_name()),
            "phone_personal": fake.phone_number(),
            # ... 20+ fields with realistic data
        })
```

---
