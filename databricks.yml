bundle:
  name: "data_pipeline"

# Workspace configuration
workspace:
  host: ${workspace.host}  # Auto-populated from databricks auth

targets:
  dev_feature:
    mode: development
    workspace:
      root_path: "/Shared/dbx/projects/data-pipeline_dev_feature_v2"
    
    artifacts:
      default:
        type: whl
        build: python setup.py bdist_wheel
        path: ./dist/*.whl
    
    resources:
      jobs:
        data_pipeline-rep-smoke-dev-feature-val-v1:
          name: "data_pipeline-rep-smoke-dev-feature-val-v1"
          
          tasks:
            - task_key: "rep_dev_feature_validation_task"
              job_cluster_key: "default_cluster"
              
              python_wheel_task:
                package_name: "data_pipeline"
                entry_point: "data-pipeline-etl"
                parameters:
                  - "data_pipeline.validation.task.rep_val"
                  - "--env"
                  - "dev"
                  - "--space"
                  - "feature"
                  - "--object_type"
                  - "re"
                  - "--job_type"
                  - "rep"
                  - "--test_type"
                  - "smoke"
              
              libraries:
                - whl: ../dist/*.whl  # DAB auto-uploads this
          
          # job_clusters:
          #   - job_cluster_key: "default_cluster"
          #     new_cluster:
          #       spark_version: "13.3.x-scala2.12"
          #       node_type_id: "Standard_DS3_v2"
          #       num_workers: 1
          #       spark_conf:
          #         "spark.databricks.cluster.profile": "serverless"
          #         "spark.master": "local[*, 4]"
          
          timeout_seconds: 3600