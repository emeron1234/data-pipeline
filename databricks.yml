bundle:
  name: "data_pipeline"

# Workspace configuration is handled by DATABRICKS_HOST environment variable
# No need to specify workspace.host here - it's auto-detected from authentication

targets:
  dev_feature:
    mode: development
    workspace:
      root_path: "~/.bundle/${bundle.name}/${bundle.target}"
    
    # Note: Wheel is built by GitHub Actions before DAB runs
    # artifacts:
    #   default:
    #     type: whl
    #     build: python setup.py bdist_wheel
    #     path: ./dist
    
    resources:
      jobs:
        data_pipeline-rep-smoke-dev-feature-val-v1:
          name: "data_pipeline-rep-smoke-dev-feature-val-v1"
          
          tasks:
            - task_key: "rep_dev_feature_validation_task"
              
              python_wheel_task:
                package_name: "data_pipeline"
                entry_point: "data-pipeline-etl"
                parameters:
                  - "data_pipeline.validation.task.rep_val"
                  - "--env"
                  - "dev"
                  - "--space"
                  - "feature"
                  - "--object_type"
                  - "re"
                  - "--job_type"
                  - "rep"
                  - "--test_type"
                  - "smoke"
              
              libraries:
                - whl: ./dist/*.whl  # DAB auto-uploads this
          
          # Use serverless compute (no job_clusters needed)
          
          timeout_seconds: 3600