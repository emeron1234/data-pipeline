## Serverless-compatible job deployment for validation (dev_feature)
# This file is tailored for workspaces that only allow Serverless (no classic clusters).
# Key points:
# - Uses job_clusters with a serverless single-node spec (num_workers: 0)
# - Adds spark.databricks.cluster.profile=serverless and node_type_id=serverless
# - Includes PHOTON runtime_engine for better performance (optional)
# - Keeps parameters consistent with existing validation tasks

environments:
  dev_feature:
    workflows:
      - name: "we-pipeline-rep-smoke-dev-feature-val-v1"
        job_clusters:
          - job_cluster_key: "serverless_cluster"
            new_cluster:
              spark_version: "13.3.x-scala2.12"
              node_type_id: "Standard_DS3_v2"
              num_workers: 1
              spark_conf:
                "spark.databricks.cluster.profile": "serverless"

        tasks:
          - task_key: "rep_dev_feature_validation_task"
            job_cluster_key: "serverless_cluster"

            spark_python_task:
              python_file: "file://main.py"
              parameters: [ "data_pipeline.validation.task.rep_val", "--env", "dev", "--space", "feature", "--zone", "raw", "--object_type", "re", "--job_type", "rep", "--test_type", "smoke" ]

        # compute:
        #   - compute_key: serverless_compute
        #     compute_type: SERVERLESS
        #     spec:
        #       kind: SERVERLESS
        #       serverless:
        #         enabled: true


