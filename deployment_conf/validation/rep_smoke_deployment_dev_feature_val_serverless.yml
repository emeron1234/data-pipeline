## Serverless-compatible job deployment for validation (dev_feature)
# This file is tailored for workspaces that only allow Serverless (no classic clusters).
# Key points:
# - Uses job_clusters with a serverless single-node spec (num_workers: 0)
# - Adds spark.databricks.cluster.profile=serverless and node_type_id=serverless
# - Includes PHOTON runtime_engine for better performance (optional)
# - Keeps parameters consistent with existing validation tasks

environments:
  dev_feature:
    workflows:
      - name: "we-pipeline-rep-smoke-dev-feature-val-v1"
        # Inline serverless cluster specification per task to avoid separate job_cluster creation
        tasks:
          - task_key: "rep_dev_feature_validation_task"
            new_cluster:
              spark_version: "15.4.x-scala2.12"  # use a more recent serverless-supported runtime (adjust if unsupported)
              num_workers: 1  # serverless often expects at least 1 worker
              runtime_engine: "PHOTON"
              spark_conf:
                spark.databricks.cluster.profile: "serverless"
                spark.databricks.adaptive.autoOptimizeShuffle.enabled: "true"
                spark.cleaner.referenceTracking.cleanCheckpoints: "true"
                spark.databricks.python.defaultPythonRepl: "pythonshell"
            spark_python_task:
              python_file: "file://main.py"
              parameters: [ "data_pipeline.validation.task.rep_val", "--env", "dev", "--space", "feature", "--zone", "raw", "--object_type", "re", "--job_type", "rep", "--test_type", "smoke" ]
          - task_key: "rep_dev_feature_validation_task_silver"
            new_cluster:
              spark_version: "15.4.x-scala2.12"
              num_workers: 1
              runtime_engine: "PHOTON"
              spark_conf:
                spark.databricks.cluster.profile: "serverless"
                spark.databricks.adaptive.autoOptimizeShuffle.enabled: "true"
                spark.cleaner.referenceTracking.cleanCheckpoints: "true"
                spark.databricks.python.defaultPythonRepl: "pythonshell"
            spark_python_task:
              python_file: "file://main.py"
              parameters: [ "data_pipeline.validation.task.rep_val", "--env", "dev", "--space", "feature", "--zone", "silver", "--object_type", "re", "--job_type", "rep", "--test_type", "smoke" ]
            depends_on:
              - task_key: "rep_dev_feature_validation_task"
            tags:
              alt:subcomponent: "rep_dev_feature_validation_task_silver"
