# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster: &basic-cluster
    new_cluster:
      spark_version: "13.3.x-scala2.12"
      # Removed policy_id for Free Edition compatibility
      # policy_id: "784e7b5d-9f6e-4570-8530-8cc78d54dc66"
      
      # Single node cluster for Free Edition (driver only)
      num_workers: 0
      # Removed autoscale for Free Edition
      # autoscale:
      #   min_workers: 4
      #   max_workers: 8
      
      # Use a node type available in Free Edition
      node_type_id: "i3.xlarge"  # Adjust based on what's available in your workspace
      
      # Removed data_security_mode for Free Edition compatibility
      # data_security_mode: "SINGLE_USER"
      
      spark_conf:
        spark.databricks.adaptive.autoOptimizeShuffle.enabled: "true"
        # Removed Unity Catalog config for Free Edition
        # spark.databricks.unityCatalog.userIsolation.python.preview: "true"
        spark.cleaner.referenceTracking.cleanCheckpoints: "true"
        spark.hadoop.fs.s3a.multipart.size: 104857600
        spark.databricks.python.defaultPythonRepl: "pythonshell"

  # Removed ACL for Free Edition compatibility
  # acl: &acl
  #   access_control_list:
  #     - service_principal_name: "98901d96-1aac-476a-a41f-08ebc08cb6af"
  #       permission_level: "IS_OWNER"
  #     - group_name: "SG-Altrata-Databricks-Development-Admin"
  #       permission_level: "CAN_MANAGE"
  #     - group_name: "SG-Altrata-Databricks-Development-Users"
  #       permission_level: "CAN_MANAGE_RUN"

environments:
  dev_feature:
    workflows:
      - name: "we-pipeline-rep-smoke-dev-feature-val-v1"
        # Removed ACL reference for Free Edition
        # <<: *acl
        job_clusters:
          - job_cluster_key: "default"
            <<: *basic-cluster
        tasks:
          - task_key: "rep_dev_feature_validation_task"
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://main.py"
              parameters: [ "data_pipeline.validation.task.rep_val", "--env", "dev", "--space", "feature", "--zone", "raw", "--object_type", "re", "--job_type", "rep", "--test_type", "smoke" ]
            # email_notifications:
            #   on_success:
            #   - jerry.siow@altrata.com
            #   - abdulhaziq.matlan@altrata.com
            #   on_failure:
            #   - jerry.siow@altrata.com
            #   - abdulhaziq.matlan@altrata.com
              
          - task_key: "rep_dev_feature_validation_task_silver"
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://main.py"
              parameters: [ "data_pipeline.validation.task.rep_val", "--env", "dev", "--space", "feature", "--zone", "silver", "--object_type", "re", "--job_type", "rep", "--test_type", "smoke" ]
            depends_on:
              - task_key: "rep_dev_feature_validation_task"
            tags:
              "alt:subcomponent": "rep_dev_feature_validation_task_silver"
            # email_notifications:
            #   on_success:
            #   - jerry.siow@altrata.com
            #   - abdulhaziq.matlan@altrata.com
            #   on_failure:
            #   - jerry.siow@altrata.com
            #   - abdulhaziq.matlan@altrata.com