name: QA Validation Job

on:
  workflow_call:
    inputs:
      environment:
        type: string
        required: true
        description: Deployment environment
      space:
        type: string
        required: true
        description: Space
      object_type:
        type: string
        required: true
        description: Object Type
      job_type:
        type: string
        required: true
        description: Job type
      test_type:
        type: string
        required: true
        description: Test type
      deploy:
        type: boolean
        required: true
        description: Deploy a workflow before launching it
      skip_test:
        type: boolean
        description: Skip test
      deployment_file:
        type: string
        required: true
        description: File deployment
    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      GH_TOKEN:
        required: true

jobs:
  qa-deploy-config:
    environment: ${{ inputs.environment }}_${{ inputs.space }}
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      GH_TOKEN: ${{ secrets.GH_TOKEN }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"
      # # Testing ----------------------------
      # # Check DATABRICKS_HOST
      # - name: Check DATABRICKS_HOST host resolves
      #   run: |
      #     if [[ ! "$DATABRICKS_HOST" =~ ^https?:// ]]; then
      #       echo "Error: DATABRICKS_HOST must start with http:// or https://"
      #       exit 1
      #     fi
      #     curl -Is --fail "$DATABRICKS_HOST" || { echo "‚ùå Host not reachable"; exit 1; }
      #     echo "‚úÖ Host is reachable"
      
      # # Check DATABRICKS_TOKEN
      # - name: Check DATABRICKS_TOKEN authentication
      #   run: |
      #     response=$(curl -s -o /dev/null -w "%{http_code}" \
      #       -H "Authorization: Bearer $DATABRICKS_TOKEN" \
      #       "$DATABRICKS_HOST/api/2.0/clusters/list")

      #     if [ "$response" -eq 200 ]; then
      #       echo "‚úÖ DATABRICKS_TOKEN is valid."
      #     elif [ "$response" -eq 401 ]; then
      #       echo "‚ùå Unauthorized: DATABRICKS_TOKEN is invalid or expired."
      #       exit 1
      #     else
      #       echo "‚ö†Ô∏è Unexpected response code: $response"
      #       exit 1
      #     fi

      # # Check GH_TOKEN
      # - name: Check GH_TOKEN authentication
      #   run: |
      #     user=$(curl -s -H "Authorization: token $GH_TOKEN" https://api.github.com/user | jq -r '.login')
      #     if [ "$user" = "null" ] || [ -z "$user" ]; then
      #       echo "‚ùå GH_TOKEN invalid or expired."
      #       exit 1
      #     else
      #       echo "‚úÖ Authenticated as: $user"
      #     fi
      # # Check GH_TOKEN User
      # - name: Check GitHub user
      #   run: |
      #     curl -s -H "Authorization: token $GH_TOKEN" https://api.github.com/user | jq
      
      # - name: Check if secrets are available
      #   run: |
      #     if [ -z "${{ secrets.GH_TOKEN }}" ]; then
      #       echo "Secrets not set properly. ${{ secrets.GH_TOKEN }} "
      #       exit 1
      #     else
      #       echo "Secrets are set"
      #     fi
      # # Testing ----------------------------
      # - name: Upload config file to Unity Catalog Volume (overwrite-safe)
      #   run: |
      #     set -e
      #     file_path="/Volumes/data_lake_dev/feature_config/volume/data_pipeline_config_dev.json"

      #     echo "üßπ Deleting existing file if present..."
      #     curl -s -X DELETE -H "Authorization: Bearer $DATABRICKS_TOKEN" \
      #       "$DATABRICKS_HOST/api/2.1/unity-catalog/volumes/files/delete?path=${file_path}" || true

      #     echo "‚¨ÜÔ∏è Uploading configuration file..."
      #     status=$(curl -s -o /dev/null -w "%{http_code}" \
      #       -X POST -H "Authorization: Bearer $DATABRICKS_TOKEN" \
      #       -F path="${file_path}" \
      #       -F contents=@data_pipeline/core/config/data_pipeline_config_dev.json \
      #       "$DATABRICKS_HOST/api/2.1/unity-catalog/volumes/files/upload")

      #     if [ "$status" -eq 200 ]; then
      #       echo "‚úÖ File uploaded successfully to ${file_path}"
      #     else
      #       echo "‚ùå Upload failed with status code: $status"
      #       exit 1
      #     fi

      #   # databricks fs cp --overwrite data_pipeline/core/config/data_pipeline_config_dev.json /Volumes/data_lake_dev/feature_config/volume/data_pipeline_config_dev.json


      # - name: Install latest dbx
      #   run: |
      #     pip install --upgrade pip
      #     pip install "dbx>=0.8.19" --no-cache-dir

      - name: Select deployment file (serverless fallback)
        run: |
          DEPLOYMENT_FILE="${{ inputs.deployment_file }}"
          SERVERLESS_FILE="./deployment_conf/validation/rep_smoke_deployment_dev_feature_val.yml"
          MINIMAL_FILE="./deployment_conf/validation/rep_smoke_deployment_dev_feature_val_minimal.yml"
          
          if [ -f "$DEPLOYMENT_FILE" ]; then
            echo "DEPLOYMENT_FILE=$DEPLOYMENT_FILE" >> $GITHUB_ENV
            echo "Using provided deployment file: $DEPLOYMENT_FILE"
          elif [ -f "$SERVERLESS_FILE" ]; then
            echo "DEPLOYMENT_FILE=$SERVERLESS_FILE" >> $GITHUB_ENV
            echo "Using serverless deployment file: $SERVERLESS_FILE"
          elif [ -f "$MINIMAL_FILE" ]; then
            echo "DEPLOYMENT_FILE=$MINIMAL_FILE" >> $GITHUB_ENV
            echo "Using minimal serverless deployment file: $MINIMAL_FILE"
          else
            echo "‚ùå No suitable deployment file found"
            exit 1
          fi

      - name: Validate runtime version
        run: |
          echo "Checking available Spark versions for serverless..."
          curl -s -H "Authorization: Bearer $DATABRICKS_TOKEN" \
            "$DATABRICKS_HOST/api/2.0/clusters/spark-versions" | \
            jq -r '.spark_versions[] | select(.name | contains("LTS")) | .key' | head -5

      - name: Deploy Integration Test Suite
        run: |
          echo "Deploying with file: $DEPLOYMENT_FILE"
          dbx deploy --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 \
            --deployment-file="$DEPLOYMENT_FILE" \
            --environment ${{ inputs.environment }}_${{ inputs.space }} \
            --debug
      - name: Run Integration test suite for cross_platform_count
        run: |
            dbx launch --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 --trace --environment ${{ inputs.environment }}_${{ inputs.space }} --parameters='{"python_params": ["we.pipeline.validation.task.${{ inputs.job_type }}_val", "--env", "'${{ inputs.environment }}'", "--space", "'${{ inputs.space }}'", "--object_type", "'${{ inputs.object_type }}'", "--job_type", "'${{ inputs.job_type }}'", "--test_type", "'${{ inputs.test_type }}'"]}'