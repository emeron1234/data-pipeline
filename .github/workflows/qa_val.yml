name: QA Validation Job

on:
  workflow_call:
    inputs:
      environment:
        type: string
        required: true
        description: Deployment environment
      space:
        type: string
        required: true
        description: Space
      object_type:
        type: string
        required: true
        description: Object Type
      job_type:
        type: string
        required: true
        description: Job type
      test_type:
        type: string
        required: true
        description: Test type
      deploy:
        type: boolean
        required: true
        description: Deploy a workflow before launching it
      skip_test:
        type: boolean
        description: Skip test
      deployment_file:
        type: string
        required: true
        description: File deployment
    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      GH_TOKEN:
        required: true


jobs:
  qa-deploy-config:
    environment: ${{ inputs.environment }}_${{ inputs.space }}
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      GH_TOKEN: ${{ secrets.GH_TOKEN }}
      # Set MLflow to use default artifact location to avoid DBFS root issues
      MLFLOW_DEFAULT_ARTIFACT_ROOT: ""
      # Force experiment name to a stable /Shared path (experiment will be created automatically if absent)
      MLFLOW_EXPERIMENT_NAME: "/Shared/dbx/projects/we-pipeline_dev_feature"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"
          # Ensure mlflow client available (already in dev extras but force just in case)
          pip install mlflow
      - name: Enable MLflow artifact monkeypatch
        run: |
          echo "Adding pythonpath_patches to PYTHONPATH for sitecustomize monkeypatch"
          echo "PYTHONPATH=$PWD/pythonpath_patches:${PYTHONPATH}" >> $GITHUB_ENV
      - name: Show configured project.json before configure
        run: |
          echo "--- Current project.json (pre-configure) ---"; cat .dbx/project_filestore.json || true
      - name: Cleanup stale MLflow experiments
        run: |
          python scripts/cleanup_mlflow_experiment.py || echo "Cleanup script finished (non-fatal warnings ignored)"
      - name: Configure dbx for Free Edition
        run: |
          echo "Trying different dbx configurations for Free Edition compatibility..."
          
          # First, try FileStore configuration (most compatible with Free Edition)
          echo "Testing FileStore configuration..."
          # Always start from bundled FileStore config (we've unified configs to FileStore path)
          cp .dbx/project_filestore.json .dbx/project.json
          if dbx configure --profile DEFAULT --environment dev_feature 2>/dev/null; then
            echo "✅ FileStore configuration works"
          else
            echo "❌ FileStore configuration failed, trying tmp directory..."
            # Fallback to tmp directory
            cp .dbx/project_free_edition.json .dbx/project.json
            if dbx configure --profile DEFAULT --environment dev_feature 2>/dev/null; then
              echo "✅ Tmp directory configuration works"
            else
              echo "❌ Tmp directory failed, trying user-specific path..."
              # Last resort: user-specific path
              # Replace ${USER} with actual user from Databricks
              sed "s/\${USER}/$(echo $DATABRICKS_HOST | cut -d'/' -f3 | cut -d'.' -f1)/g" .dbx/project_user_specific.json > .dbx/project.json
              echo "✅ Using user-specific configuration"
            fi
          fi
          
          echo "Final configuration:"
          cat .dbx/project.json
      - name: Select deployment file (fallback to serverless)
        id: select-deployment
        run: |
          INPUT_FILE='${{ inputs.deployment_file }}'
          SERVERLESS_FILE=./deployment_conf/validation/rep_smoke_deployment_dev_feature_val_serverless.yml
          if grep -q 'serverless' <<<"$INPUT_FILE"; then
            echo "Using provided serverless deployment file: $INPUT_FILE"
            echo "DEPLOYMENT_FILE=$INPUT_FILE" >> $GITHUB_ENV
          elif grep -q 'free_edition' <<<"$INPUT_FILE"; then
            echo "'free_edition' file specified but workspace is serverless-only; forcing serverless deployment file"
            echo "DEPLOYMENT_FILE=$SERVERLESS_FILE" >> $GITHUB_ENV
          elif [ -f "$INPUT_FILE" ]; then
            echo "Using specified deployment file: $INPUT_FILE (no explicit serverless token, overriding to serverless for safety)";
            echo "DEPLOYMENT_FILE=$SERVERLESS_FILE" >> $GITHUB_ENV
          else
            echo "Specified file not found, defaulting to serverless deployment config.";
            echo "DEPLOYMENT_FILE=$SERVERLESS_FILE" >> $GITHUB_ENV
          fi
          echo "Chosen deployment file: $(grep DEPLOYMENT_FILE $GITHUB_ENV)"
      # - name: Upload configuration file
      # # data_pipeline\core\config\data_pipeline_config_dev.json
      #   run: |
      #     databricks fs cp --overwrite data_pipeline/core/config/data_pipeline_config_${{ inputs.environment }}.json /Volumes/data_lake_dev/feature_silver_master/test_table/data_pipeline_config_${{ inputs.environment }}.json
      #   # read this: https://medium.com/towards-data-engineering/when-dbfs-is-not-an-option-how-databricks-volumes-saved-the-day-for-file-based-learning-on-free-a0e0e92579ce
      - name: Deploy Integration test suite other than ddp
        if: ${{ inputs.job_type != 'ddp' }}
        run: |
          echo "Deployment file: $DEPLOYMENT_FILE"; ls -l "$DEPLOYMENT_FILE" || (echo "Deployment file missing" && exit 1)
          dbx deploy --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 --deployment-file=$DEPLOYMENT_FILE --environment ${{ inputs.environment }}_${{ inputs.space }} --debug

      - name: Deploy Integration test suite ddp
        if: ${{ inputs.job_type == 'ddp' }}
        run: |
          echo "Deployment file: $DEPLOYMENT_FILE"; ls -l "$DEPLOYMENT_FILE" || (echo "Deployment file missing" && exit 1)
          dbx deploy --job=ddp-val-wf --deployment-file=$DEPLOYMENT_FILE --environment ${{ inputs.environment }}_${{ inputs.space }} --debug

      - name: Run Integration test suite for cross_platform_count
        if: ${{ inputs.job_type == 'cross_platform_count' }}
        run: |
            dbx launch --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 --trace --environment ${{ inputs.environment }}_${{ inputs.space }} --parameters='{"python_params": ["we.pipeline.validation.task.${{ inputs.job_type }}_val", "--env", "'${{ inputs.environment }}'", "--space", "'${{ inputs.space }}'", "--object_type", "'${{ inputs.object_type }}'", "--job_type", "'${{ inputs.job_type }}'", "--test_type", "'${{ inputs.test_type }}'"]}'