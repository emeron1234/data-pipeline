name: QA Validation Job

on:
  workflow_call:
    inputs:
      environment:
        type: string
        required: true
        description: Deployment environment
      space:
        type: string
        required: true
        description: Space
      object_type:
        type: string
        required: true
        description: Object Type
      job_type:
        type: string
        required: true
        description: Job type
      test_type:
        type: string
        required: true
        description: Test type
      deploy:
        type: boolean
        required: true
        description: Deploy a workflow before launching it
      skip_test:
        type: boolean
        description: Skip test
      deployment_file:
        type: string
        required: true
        description: File deployment
    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      GH_TOKEN:
        required: true

jobs:
  qa-deploy-config:
    environment: ${{ inputs.environment }}_${{ inputs.space }}
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      GH_TOKEN: ${{ secrets.GH_TOKEN }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"

      # -----------
      - name: Upgrade Databricks CLI
        run: |
          pip install "databricks-cli>=0.270.0"
      - name: Verify Databricks CLI Version
        run: |
          databricks --version
      - name: Build Wheel
        run: |
          python setup.py bdist_wheel
      - name: Deploy Bundle
        run: |
          databricks bundle deploy -t dev_feature

      - name: Configure Databricks CLI
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg
          echo "token = $DATABRICKS_TOKEN" >> ~/.databrickscfg

      - name: Run Job
        run: |
          databricks bundle run -t dev_feature data_pipeline-rep-smoke-dev-feature-val-v1 \
          --parameters='{"parameters": ["data_pipeline.validation.task.rep_val", "--env", "dev", "--space", "feature", "--object_type", "re", "--job_type", "rep", "--test_type", "smoke"]}'

      # -----------
      

      # - name: Deploy Integration Test Suite
      #   run: |
      #     DEPLOYMENT_FILE="${{ inputs.deployment_file }}"
      #     echo "Deploying with file: $DEPLOYMENT_FILE"
      #     dbx deploy --job=data_pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 \
      #       --deployment-file="$DEPLOYMENT_FILE" \
      #       --environment ${{ inputs.environment }}_${{ inputs.space }} \
      #       --no-rebuild \
      #       --no-package \
      #       --debug

      # - name: Run Integration Test Suite
      #   run: |
      #     echo "Launch file: $DEPLOYMENT_FILE"
      #     dbx launch --job=data_pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 \
      #     --trace --environment ${{ inputs.environment }}_${{ inputs.space }} \
      #     --parameters='{"python_params": ["data_pipeline.validation.task.${{ inputs.job_type }}_val", "--env", "'${{ inputs.environment }}'", "--space", "'${{ inputs.space }}'", "--object_type", "'${{ inputs.object_type }}'", "--job_type", "'${{ inputs.job_type }}'", "--test_type", "'${{ inputs.test_type }}'"]}'