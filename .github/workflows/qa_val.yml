name: QA Validation Job

on:
  workflow_call:
    inputs:
      environment:
        type: string
        required: true
        description: Deployment environment
      space:
        type: string
        required: true
        description: Space
      object_type:
        type: string
        required: true
        description: Object Type
      job_type:
        type: string
        required: true
        description: Job type
      test_type:
        type: string
        required: true
        description: Test type
      deploy:
        type: boolean
        required: true
        description: Deploy a workflow before launching it
      skip_test:
        type: boolean
        description: Skip test
      deployment_file:
        type: string
        required: true
        description: File deployment
    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      GH_TOKEN:
        required: true

jobs:
  qa-deploy-config:
    environment: ${{ inputs.environment }}_${{ inputs.space }}
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      GH_TOKEN: ${{ secrets.GH_TOKEN }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"
      # Testing ----------------------------
      # Check DATABRICKS_HOST
      - name: Check DATABRICKS_HOST host resolves
        run: |
          if [[ ! "$DATABRICKS_HOST" =~ ^https?:// ]]; then
            echo "Error: DATABRICKS_HOST must start with http:// or https://"
            exit 1
          fi
          curl -Is --fail "$DATABRICKS_HOST" || { echo "❌ Host not reachable"; exit 1; }
          echo "✅ Host is reachable"
      
      # Check DATABRICKS_TOKEN
      - name: Check DATABRICKS_TOKEN authentication
        run: |
          response=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "Authorization: Bearer $DATABRICKS_TOKEN" \
            "$DATABRICKS_HOST/api/2.0/clusters/list")

          if [ "$response" -eq 200 ]; then
            echo "✅ DATABRICKS_TOKEN is valid."
          elif [ "$response" -eq 401 ]; then
            echo "❌ Unauthorized: DATABRICKS_TOKEN is invalid or expired."
            exit 1
          else
            echo "⚠️ Unexpected response code: $response"
            exit 1
          fi

      # Check GH_TOKEN
      - name: Check GH_TOKEN authentication
        run: |
          user=$(curl -s -H "Authorization: token $GH_TOKEN" https://api.github.com/user | jq -r '.login')
          if [ "$user" = "null" ] || [ -z "$user" ]; then
            echo "❌ GH_TOKEN invalid or expired."
            exit 1
          else
            echo "✅ Authenticated as: $user"
          fi
      # Check GH_TOKEN User
      - name: Check GitHub user
        run: |
          curl -s -H "Authorization: token $GH_TOKEN" https://api.github.com/user | jq
      
      - name: Check if secrets are available
        run: |
          if [ -z "${{ secrets.GH_TOKEN }}" ]; then
            echo "Secrets not set properly. ${{ secrets.GH_TOKEN }} "
            exit 1
          else
            echo "Secrets are set"
          fi
      # Testing ----------------------------
      - name: Configure Databricks CLI
        run: |
          export DATABRICKS_HOST=$DATABRICKS_HOST
          export DATABRICKS_TOKEN=$DATABRICKS_TOKEN
          databricks fs cp --overwrite data_pipeline/core/config/data_pipeline_config_dev.json /Volumes/data_lake_dev/feature_config/volume/data_pipeline_config_dev.json
        # databricks fs cp --overwrite data_pipeline/core/config/data_pipeline_config_dev.json dbfs:/FileStore/configuration/data_pipeline_config_dev.json




      - name: Upload configuration file
        run: |
          databricks fs cp --overwrite data_pipeline/core/config/data_pipeline_config_dev.json /Volumes/data_lake_dev/feature_config/volume/data_pipeline_config_dev.json
# databricks fs cp --overwrite data_pipeline/core/config/data_pipeline_config_${{ inputs.environment }}.json dbfs:/FileStore/configuration/data_pipeline_config_${{ inputs.environment }}.json
      - name: Deploy Integration test suite other than ddp
        if: ${{ inputs.job_type != 'ddp' }}
        run: |
          dbx deploy --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 --deployment-file=${{ inputs.deployment_file }} --environment ${{ inputs.environment }}_${{ inputs.space }} --debug

      # - name: Deploy Integration test suite ddp
      #   if: ${{ inputs.job_type == 'ddp' }}
      #   run: |
      #     dbx deploy --job=ddp-val-wf --deployment-file=${{ inputs.deployment_file }} --environment ${{ inputs.environment }}_${{ inputs.space }} --debug

      # - name: Run Integration test suite for cross_platform_count
      #   if: ${{ inputs.job_type == 'cross_platform_count' }}
      #   run: |
      #       dbx launch --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 --trace --environment ${{ inputs.environment }}_${{ inputs.space }} --parameters='{"python_params": ["we.pipeline.validation.task.${{ inputs.job_type }}_val", "--env", "'${{ inputs.environment }}'", "--space", "'${{ inputs.space }}'", "--object_type", "'${{ inputs.object_type }}'", "--job_type", "'${{ inputs.job_type }}'", "--test_type", "'${{ inputs.test_type }}'"]}'