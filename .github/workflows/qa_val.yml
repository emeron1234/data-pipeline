name: QA Validation Job

on:
  workflow_call:
    inputs:
      environment:
        type: string
        required: true
        description: Deployment environment
      space:
        type: string
        required: true
        description: Space
      object_type:
        type: string
        required: true
        description: Object Type
      job_type:
        type: string
        required: true
        description: Job type
      test_type:
        type: string
        required: true
        description: Test type
      deploy:
        type: boolean
        required: true
        description: Deploy a workflow before launching it
      skip_test:
        type: boolean
        description: Skip test
      deployment_file:
        type: string
        required: true
        description: File deployment
    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      GH_TOKEN:
        required: true


jobs:
  qa-deploy-config:
    environment: ${{ inputs.environment }}_${{ inputs.space }}
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      GH_TOKEN: ${{ secrets.GH_TOKEN }}
      # Set MLflow to use default artifact location to avoid DBFS root issues
      MLFLOW_DEFAULT_ARTIFACT_ROOT: ""
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"
      - name: Configure dbx for Free Edition
        run: |
          echo "Trying different dbx configurations for Free Edition compatibility..."
          
          # First, try FileStore configuration (most compatible with Free Edition)
          echo "Testing FileStore configuration..."
          cp .dbx/project_filestore.json .dbx/project.json
          if dbx configure --profile DEFAULT --environment dev_feature 2>/dev/null; then
            echo "✅ FileStore configuration works"
          else
            echo "❌ FileStore configuration failed, trying tmp directory..."
            # Fallback to tmp directory
            cp .dbx/project_free_edition.json .dbx/project.json
            if dbx configure --profile DEFAULT --environment dev_feature 2>/dev/null; then
              echo "✅ Tmp directory configuration works"
            else
              echo "❌ Tmp directory failed, trying user-specific path..."
              # Last resort: user-specific path
              # Replace ${USER} with actual user from Databricks
              sed "s/\${USER}/$(echo $DATABRICKS_HOST | cut -d'/' -f3 | cut -d'.' -f1)/g" .dbx/project_user_specific.json > .dbx/project.json
              echo "✅ Using user-specific configuration"
            fi
          fi
          
          echo "Final configuration:"
          cat .dbx/project.json
      # - name: Upload configuration file
      # # data_pipeline\core\config\data_pipeline_config_dev.json
      #   run: |
      #     databricks fs cp --overwrite data_pipeline/core/config/data_pipeline_config_${{ inputs.environment }}.json /Volumes/data_lake_dev/feature_silver_master/test_table/data_pipeline_config_${{ inputs.environment }}.json
      #   # read this: https://medium.com/towards-data-engineering/when-dbfs-is-not-an-option-how-databricks-volumes-saved-the-day-for-file-based-learning-on-free-a0e0e92579ce
      - name: Deploy Integration test suite other than ddp
        if: ${{ inputs.job_type != 'ddp' }}
        run: |
          dbx deploy --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 --deployment-file=${{ inputs.deployment_file }} --environment ${{ inputs.environment }}_${{ inputs.space }} --debug

      - name: Deploy Integration test suite ddp
        if: ${{ inputs.job_type == 'ddp' }}
        run: |
          dbx deploy --job=ddp-val-wf --deployment-file=${{ inputs.deployment_file }} --environment ${{ inputs.environment }}_${{ inputs.space }} --debug

      - name: Run Integration test suite for cross_platform_count
        if: ${{ inputs.job_type == 'cross_platform_count' }}
        run: |
            dbx launch --job=we-pipeline-${{ inputs.job_type }}-${{ inputs.test_type }}-${{ inputs.environment }}-${{ inputs.space }}-val-v1 --trace --environment ${{ inputs.environment }}_${{ inputs.space }} --parameters='{"python_params": ["we.pipeline.validation.task.${{ inputs.job_type }}_val", "--env", "'${{ inputs.environment }}'", "--space", "'${{ inputs.space }}'", "--object_type", "'${{ inputs.object_type }}'", "--job_type", "'${{ inputs.job_type }}'", "--test_type", "'${{ inputs.test_type }}'"]}'